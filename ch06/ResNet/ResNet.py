'''
ResNetï¼ˆResidual Networkï¼‰= â€œå¸¦æ·å¾„çš„æ·±ç½‘â€ã€‚æ ¸å¿ƒç‚¹æ˜¯æ®‹å·®è¿æ¥ï¼ˆskip/shortcutï¼‰
è®©æ¯ä¸ªå—å­¦ä¹  (ğ‘¥)åä¸è¾“å…¥ x ç›¸åŠ ï¼Œè¾“å‡º y=F(x)+xã€‚è¿™æ ·æ¢¯åº¦å¯ä»¥ç›´æ¥ç©¿è¿‡æ’ç­‰æ˜ å°„å›æµ
'''

import torch
from torch import nn
from torch.nn import functional as F

from LM.utils import load_data_fashion_mnist, train_ch6

"""
1Ã—1 å·ç§¯çš„æ ¸å¿ƒä½œç”¨æ˜¯å¯¹â€œé€šé“ç»´åº¦â€åšå¯å­¦ä¹ çš„çº¿æ€§å˜æ¢ï¼šåœ¨ä¸æ”¹å˜ç©ºé—´å°ºå¯¸ï¼ˆstride=1, padding=0ï¼‰çš„æƒ…å†µä¸‹ï¼Œ
å¯¹æ¯ä¸ªåƒç´ ä½ç½®æŒ‰é€šé“è¿›è¡Œé‡ç»„ã€‚ä¸»è¦ç”¨é€”ï¼š
â‘  é€šé“å˜æ¢ï¼ˆå‡/é™ç»´ï¼‰ï¼Œåœ¨ ResNet çš„æŠ•å½±æ·å¾„ä¸­ç”¨ 1Ã—1 å°†è¾“å…¥çš„é€šé“æ•°å’Œæ­¥å¹…å¯¹é½ä¸»åˆ†æ”¯ï¼Œä¿è¯èƒ½åš F(x)+xï¼›
â‘¡ ç“¶é¢ˆé™ç®—åŠ›ï¼Œå…ˆç”¨ 1Ã—1 å°†é€šé“ä» C é™åˆ° Câ€² å†åš 3Ã—3 å·ç§¯ï¼Œå‚æ•°/ç®—åŠ›æ˜¾è‘—é™ä½ï¼ˆå¦‚ C=256ã€Câ€²=64 æ—¶çº¦å¯èŠ‚çœ 8Ã— ä»¥ä¸Šï¼‰ï¼›
â‘¢ è·¨é€šé“ç‰¹å¾é‡æ··åˆï¼ŒåŒ¹é… BN/æ¿€æ´»åæå‡è¡¨è¾¾åŠ›ï¼ˆNetwork-in-Network æ€æƒ³ï¼‰ï¼›
â‘£ å½¢çŠ¶å¯¹é½/ä¸‹é‡‡æ ·ï¼Œè®¾ç½® stride=2 çš„ 1Ã—1 åœ¨æ·å¾„é‡ŒåŒæ­¥ä¸‹é‡‡æ ·å¹¶åŒ¹é…é€šé“ï¼›
â‘¤ Inception ä¸­çš„é™ç»´ï¼Œå…ˆ 1Ã—1 é™ç»´å†æ¥ 3Ã—3/5Ã—5 åˆ†æ”¯ä»¥å®ç°å¤šå°ºåº¦ä¸”çœç®—åŠ›ã€‚è¦ç‚¹ï¼š1Ã—1 ä¸æ‰©å±•æ„Ÿå—é‡ï¼Œ
ä½†èƒ½é«˜æ•ˆé‡æ’é€šé“å¹¶é…åˆéçº¿æ€§æå‡è¡¨ç¤ºèƒ½åŠ›ï¼Œæ˜¯ ResNet/GoogLeNet ç­‰æ¶æ„ä¸­çš„å…³é”®ç§¯æœ¨ã€‚
"""
class Residual(nn.Module):
    def __init__(self, in_channel, num_channel, use_1x1conv=False, strides=1):
        '''
        :param in_channel:è¾“å…¥é€šé“æ•°
        :param num_channel:å—å†…è¾“å‡ºé€šé“æ•°
        :param use_1x1conv:æ˜¯å¦ç”¨ 1Ã—1 æŠ•å½±æ·å¾„æŠŠxçš„å½¢çŠ¶å˜æˆä¸æ®‹å·®åˆ†æ”¯ä¸€è‡´
        :param strides:
        '''
        super(Residual, self).__init__()
        self.conv1 = nn.Conv2d(in_channel, num_channel,
                               kernel_size=3,
                               stride=strides,
                               padding=1)
        self.conv2 = nn.Conv2d(num_channel, num_channel,
                               kernel_size=3,
                               padding = 1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(in_channel, num_channel,
                                   kernel_size=1,
                                   stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channel)
        self.bn2 = nn.BatchNorm2d(num_channel)
        self.relu = nn.ReLU(inplace=True) #inplace åŸåœ°æ“ä½œï¼ŒèŠ‚çº¦å†…å­˜

    def forward(self, X):
        Y= self.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X= self.conv3(X)
        Y += X
        return self.relu(Y)

# è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸€è‡´
blk = Residual(3, 3)
X = torch.rand(4, 3, 6, 6)
Y = blk(X)
# print(Y.shape) # torch.Size([4, 3, 6, 6])


#å¢åŠ è¾“å‡ºé€šé“æ•°çš„åŒæ—¶ï¼Œå‡åŠè¾“å‡ºçš„Hå’ŒW
blk = Residual(3, 6, use_1x1conv=True, strides = 2)
# print(blk(X).shape) # torch.Size([4, 6, 3, 3])
'''
ä¸ºä»€ä¹ˆè¦é€šé“æ•°ç¿»å€ï¼ŒHWå‡åŠï¼š
æ›´å¤§æ„Ÿå—é‡ï¼šä¸‹é‡‡æ ·ï¼ˆstride=2ï¼‰è®©åç»­å·ç§¯çœ‹åˆ°æ›´å¤§çš„ä¸Šä¸‹æ–‡ã€‚
è¡¨ç¤ºèƒ½åŠ›ä¸æ‰ï¼šè™½ç„¶ HÃ—W å˜å°äº†ï¼Œä½†æŠŠ é€šé“æ•°ç¿»å€ å¢å¼ºé€šé“ç»´çš„è¡¨è¾¾ï¼Œä½¿æ€»ä¿¡æ¯é‡ä¸è‡³äºä¸‹é™ã€‚
ç®—åŠ›å‡è¡¡ï¼šç©ºé—´å‡åŠï¼ˆâ‰ˆ1/4 åƒç´ æ•°ï¼‰ï¼Œé€šé“ç¿»å€ï¼ˆÃ—2ï¼‰ï¼Œæ€»ä½“å¼ é‡å…ƒç´ æ•°çº¦å‡åŠï¼›å·ç§¯çš„ FLOPs åœ¨å„ stage ä¹‹é—´æ›´å¹³è¡¡ã€‚
'''

# ResNetçš„ç¬¬ä¸€ä¸ªstageï¼Œ å¿«é€Ÿä¸‹é‡‡æ ·ï¼ˆæŠŠåˆ†è¾¨ç‡ç›´æ¥é™åˆ°åŸæ¥çš„ 1/4ï¼‰ï¼ŒæŠŠé€šé“æåˆ° 64ï¼Œæå–ä½çº§çº¹ç†ã€‚
b1 = nn.Sequential(nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),
                  nn.BatchNorm2d(64),nn.ReLU(),
                  nn.MaxPool2d(kernel_size=3,stride=2,padding=1))

# class Residualä¸ºå°blockï¼Œresnet_block ä¸ºå¤§blockï¼Œä¸ºResnetç½‘ç»œçš„ä¸€ä¸ªstage
def resnet_block(input_channels,num_channels,num_residuals,first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block: # æ¯ä¸ª stage çš„ç¬¬ä¸€ä¸ªå—ï¼ˆä¸” first_block=False æ—¶ï¼‰åœ¨å‡åŠã€‚
            blk.append(Residual(input_channels, num_channels, use_1x1conv=True,strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
    return blk

# å› ä¸ºresnet_blockè¿”å›çš„æ˜¯ä¸€ä¸ª[]æ‰€ä»¥ç”¨*è§£åŒ…-> nn.Sequential(*[m1, m2, m3]) âŸ¹ nn.Sequential(m1, m2, m3)
b2 = nn.Sequential(*resnet_block(64,64,2,first_block=True)) # å› ä¸ºb1åšäº†ä¸¤æ¬¡å®½é«˜å‡åŠï¼Œnn.Conv2dã€nn.MaxPool2dï¼Œæ‰€ä»¥b2ä¸­çš„é¦–æ¬¡å°±ä¸å‡åŠäº†
b3 = nn.Sequential(*resnet_block(64,128,2)) # b3ã€b4ã€b5çš„é¦–æ¬¡å·ç§¯å±‚éƒ½å‡åŠ
b4 = nn.Sequential(*resnet_block(128,256,2)) # çœ‹ä¸€ä¸‹è¿™ä¸ªæ˜¯ä»€ä¹ˆæ„æ€
b5 = nn.Sequential(*resnet_block(256,512,2))


net = nn.Sequential(b1,b2,b3,b4,b5,nn.AdaptiveAvgPool2d((1,1)),nn.Flatten(),nn.Linear(512,10))


# è§‚å¯Ÿä¸€ä¸‹ReNetä¸­ä¸åŒæ¨¡å—çš„è¾“å…¥å½¢çŠ¶æ˜¯å¦‚ä½•å˜åŒ–çš„
X = torch.rand(size=(1,1,224,224))
for layer in net:
    X = layer(X)
    # print(layer.__class__.__name__,'output shape:\t',X.shape) # é€šé“æ•°ç¿»å€ã€æ¨¡å‹å‡åŠ


if __name__ == '__main__':
    lr, num_epochs, batch_size = 0.0005, 1, 64
    train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    net.to(device)
    net.train()

    train_ch6(net, train_iter, test_iter, num_epochs, lr, device, True)

